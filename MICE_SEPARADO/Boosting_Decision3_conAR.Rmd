---
title: "Boosting Decision 3"
author: "Grupo 11"
date: "2025-12-13"
output: html_document
---

# Preparación de los datos

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(caret)
library(gbm)
library(xgboost)
library(dplyr)
library(pROC)
library(ROSE)
library(smotefamily)
library(DMwR2) 
```
```{r}
mydata <- read.csv("datos_relevantes_Train_Decision3.csv") 


mydata <- mydata %>% select(-ID )%>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(~ is.numeric(.) && n_distinct(.) < 20), as.factor))

mydata$Exited <- factor(mydata$Exited)


set.seed(1234)
ind <- sample(1:nrow(mydata), 0.6 * nrow(mydata))

train <- mydata[ind, ]
factor_cols <- names(train)[sapply(train, is.factor)]
test <- mydata[-ind, ]
```
```{r}
# F1 score

F1 <- function(cm) {
p <- cm$byClass["Precision"]
r <- cm$byClass["Recall"]
2*p*r/(p+r)
}

# Ajustar punto de corte

threshold_eval <- function(probs, true){
cortes <- seq(0.1, 0.9, 0.01)
f1s <- recalls <- numeric(length(cortes))

for(i in 1:length(cortes)){
pred <- ifelse(probs > cortes[i], "1","0")
cm <- confusionMatrix(factor(pred, levels=levels(true)), true, positive="1")
f1s[i] <- F1(cm)
recalls[i] <- cm$byClass["Recall"]
}

data.frame(threshold=cortes, F1=f1s, Recall=recalls)
}

```

```{r}
### OVERSAMPLING
train_over <- upSample(
  train[, -which(names(train)=="Exited")],
  train$Exited, yname="Exited"
)

### UNDERSAMPLING
train_under <- downSample(
  train[, -which(names(train)=="Exited")],
  train$Exited, yname="Exited"
)

### ROSE

bad_cols <- sapply(train, function(x) !is.numeric(x) && !is.factor(x))

train_fixed <- train %>%
  mutate(across(which(bad_cols), as.factor))

train_fixed <- train_fixed %>%
  mutate(across(where(~ is.numeric(.) && n_distinct(.) <= 20), as.factor))

train_rose <- ROSE(Exited ~ ., data=train_fixed, seed=1234)$data

```

# GBM

```{r}
trControl <- trainControl(method="cv", number=5,repeats=2)

datasets <- list(Original = train,Oversampling = train_over,Undersampling = train_under,ROSE = train_rose)


gbm_models <- list()
gbm_results <- data.frame()

for(name in names(datasets)){

set.seed(123)
model <- train(
Exited ~ ., data=datasets[[name]],
method="gbm",
trControl=trControl,
verbose=FALSE
)

pred_class <- predict(model, newdata=test)
pred_prob <- predict(model, newdata=test, type="prob")[,"1"]
cm <- confusionMatrix(pred_class, test$Exited, positive="1")

gbm_models[[name]] <- model

gbm_results <- rbind(gbm_results, data.frame(
Dataset=name,
Accuracy=cm$overall["Accuracy"],
Recall=cm$byClass["Recall"],
F1=F1(cm)
))
}

gbm_results

best_model_name <- gbm_results$Dataset[which.max(gbm_results$F1)]
best_gbm <- gbm_models[[best_model_name]]

probs_best <- predict(best_gbm, test, type="prob")[ , "1"]

eval <- threshold_eval(probs_best, test$Exited)

best_cut <- eval$threshold[ which.max(eval$F1) ]
best_cut

pred_cut <- ifelse(probs_best > best_cut, "1","0")
cm_cut <- confusionMatrix(factor(pred_cut, levels=levels(test$Exited)),
test$Exited, positive="1")
cm_cut
Accuracy<-cm_cut$overall["Accuracy"]
Recall<-cm_cut$byClass["Recall"]
precision <- cm_cut$byClass["Pos Pred Value"]
F1 <- F1(cm_cut)



cutoffs <- seq(0.1, 0.9, by = 0.005)
results <- data.frame(cutoff = numeric(), accuracy=numeric(), recall = numeric(), f1 = numeric())

for (c in cutoffs) {
  pred_corte_test <- factor(ifelse(probs_best > c, "1", "0"), levels = levels(test$Exited))
  
t2<-table(pred_corte_test, test$Exited)
t2
TP <- t2[2,2]
TN <- t2[1,1]
FP <- t2[2,1]
FN <- t2[1,2]

recall <- TP / (TP + FN)
precision <- TP / (TP + FP)
f1 <- 2 * (precision * recall) / (precision + recall)
n<-nrow(test)
accuracy2<-(sum(diag(t2))/n)
accuracy2

results <- rbind(results, data.frame(cutoff = c, accuracy=accuracy2, recall = recall, f1 = f1))
}
print(results)
```

```{r}
datos_test <- read.csv("datos_relevantes_Test.csv")

id_test <- datos_test$ID

# --- MUY IMPORTANTE ---
# 1. Convertir las columnas factor al mismo tipo
for (col in factor_cols) {
  if (col %in% names(datos_test)) {
    datos_test[[col]] <- as.factor(datos_test[[col]])
  }
}

# 2. Convertir todo lo demás a numérico
for (col in setdiff(names(datos_test), factor_cols)) {
  datos_test[[col]] <- as.numeric(datos_test[[col]])
}

# Predicción con GBM
probs_submission <- predict(best_gbm, newdata = datos_test, type = "prob")[,"1"]
pred_submission <- ifelse(probs_submission > 0.545,"Yes", "No")

# Crear submission
submission <- data.frame(
  ID = id_test,
  Exited = pred_submission
)

submission

#write.csv(submission, "Bagging_GBM_Over_0.545_Decision1.csv", row.names = FALSE)
```

## GBM SMOTE
```{r}
trControl2 <- trainControl(method="cv", number=5,sampling="smote")

set.seed(123)
gbmFit1 <- train(Exited ~ ., data = train, 
                 method = "gbm", 
                 trControl = trControl2,
                 verbose = FALSE)


cutoffs <- seq(0.1, 0.9, by = 0.005)
results <- data.frame(cutoff = numeric(), accuracy=numeric(), recall = numeric(), f1 = numeric())

probs_gbm_smote <- predict(gbmFit1, newdata=test, type="prob")[,"1"]
for (c in cutoffs) {
  pred_corte_test <- factor(ifelse(probs_gbm_smote > c, "1", "0"), levels = levels(test$Exited))
  
t2<-table(pred_corte_test, test$Exited)
t2
TP <- t2[2,2]
TN <- t2[1,1]
FP <- t2[2,1]
FN <- t2[1,2]

recall <- TP / (TP + FN)
precision <- TP / (TP + FP)
f1 <- 2 * (precision * recall) / (precision + recall)
n<-nrow(test)
accuracy2<-(sum(diag(t2))/n)
accuracy2

results <- rbind(results, data.frame(cutoff = c, accuracy=accuracy2, recall = recall, f1 = f1))
}
print(results)
```

# XGBoost

## Modelo 1 (sin balancear)
```{r}
xgb_grid <- expand.grid(nrounds = c(200, 300, 400),          
  max_depth = c(2,4, 6),        
  eta = c(0.04, 0.01, 0.05),            
  gamma = 0,
  colsample_bytree = c(0.8),
  min_child_weight = c(1, 3),     
  subsample = 0.8)
modelLookup("xgbTree")

set.seed(123)
caret.xgb <- train(
Exited ~ ., data = train,
method = "xgbTree",
trControl = trControl,
verbosity = 0, tuneGrid=xgb_grid)

caret.xgb$bestTune

pred_xgb <- predict(caret.xgb, newdata = test)
probs_xgb <- predict(caret.xgb, test, type="prob")[ , "1"]
cutoffs <- seq(0.1, 0.9, by = 0.005)
results <- data.frame(cutoff = numeric(), accuracy=numeric(), recall = numeric(), f1 = numeric())


for (c in cutoffs) {
  pred_corte_test <- factor(ifelse(probs_xgb > c, "1", "0"), levels = levels(test$Exited))
  
t2<-table(pred_corte_test, test$Exited)
t2
TP <- t2[2,2]
TN <- t2[1,1]
FP <- t2[2,1]
FN <- t2[1,2]

recall <- TP / (TP + FN)
precision <- TP / (TP + FP)
f1 <- 2 * (precision * recall) / (precision + recall)
n<-nrow(test)
accuracy2<-(sum(diag(t2))/n)
accuracy2

results <- rbind(results, data.frame(cutoff = c, accuracy=accuracy2, recall = recall, f1 = f1))
}
print(results)
```

## Modelo 2 (balanceo)
```{r}
xgb_grid <- expand.grid(nrounds = c(200, 300, 400),         
  max_depth = c(2,4, 6),         
  eta = c(0.04, 0.01, 0.05),             
  gamma = 0,
  colsample_bytree = c(0.8),
  min_child_weight = c(1, 3),    
  subsample = 0.8)

ctrl_basic <- trainControl(method = "cv", number = 5, classProbs = TRUE)
cutoffs <- seq(0.1, 0.9, by = 0.005)
results <- data.frame()

balance_types <- c("None", "Oversampling", "Undersampling", "ROSE", "Both")

train$Exited <- factor(ifelse(train$Exited == "1", "Class1", "Class0"))
test$Exited  <- factor(ifelse(test$Exited == "1", "Class1", "Class0"))


for(balance_name in balance_types){
  
  cat("Procesando balanceo:", balance_name, "\n")
  
  if(balance_name == "None"){
    train_data <- train
    ctrl <- ctrl_basic
  } else if(balance_name == "Oversampling"){
    train_data <- train
    ctrl <- trainControl(method="cv", number=5, sampling="up", classProbs = TRUE)
  } else if(balance_name == "Undersampling"){
    train_data <- train
    ctrl <- trainControl(method="cv", number=5, sampling="down", classProbs = TRUE)
  } else if(balance_name == "ROSE"){
    train_data <- ROSE(Exited ~ ., data = train, seed = 1)$data
    ctrl <- ctrl_basic
  } else if(balance_name == "Both"){
    train_data <- ovun.sample(Exited ~ ., data = train, method = "both", p=0.5, N=1000, seed = 1)$data
    ctrl <- ctrl_basic
  }
  

  set.seed(123)
  model <- train(
    Exited ~ ., data = train_data,
    method = "xgbTree",
    trControl = ctrl,
    verbosity = 0,tuneGrid=xgb_grid
  )
  
  probs <- predict(model, newdata = test, type = "prob")[, "Class1"]
  

  for(cut in cutoffs){
    pred <- factor(ifelse(probs > cut, "Class1", "Class0"), levels = levels(test$Exited))
    cm <- confusionMatrix(pred, test$Exited, positive = "Class1")
    
    precision <- cm$byClass["Pos Pred Value"]
    recall <- cm$byClass["Sensitivity"]
    F1 <- 2 * (precision * recall) / (precision + recall)
    
    results <- rbind(
      results,
      data.frame(
        Balance = balance_name,
        Cutoff = cut,
        Accuracy = cm$overall["Accuracy"],
        Recall = recall,
        F1 = F1
      )
    )
  }
}

results
```

## Modelo 3 (SMOTE)

```{r}
xgb_grid <- expand.grid(
  nrounds = c(300, 500),
  max_depth = c(2, 4),
  eta = c(0.01, 0.03),
  gamma = c(0, 0.2),
  colsample_bytree = c(0.7, 0.9),
  min_child_weight = c(1, 5),
  subsample = c(0.7, 0.9))

ctrl_SMOTE <- trainControl(method = "repeatedcv", number = 5, sampling = "smote")
set.seed(123)
modelo_smote3 <-caret::train(Exited ~ .,
                         data = train,
                         method = "xgbTree",
                         trControl = ctrl_SMOTE,tuneGrid=xgb_grid)
probs <- predict(modelo_smote3, newdata = test, type = "prob")[, "1"]

cutoffs <- seq(0.1, 0.9, by = 0.005)
results_smote <- data.frame()

for(cut in cutoffs){
  
  pred <- factor(ifelse(probs > cut, "1", "0"), levels = levels(test$Exited))
  cm <- confusionMatrix(pred, test$Exited, positive = "1")
  
  precision <- cm$byClass["Pos Pred Value"]
  recall <- cm$byClass["Sensitivity"]
  F1 <- 2 * (precision * recall) / (precision + recall)
  
  results_smote <- rbind(
    results_smote,
    data.frame(
      Balance = "SMOTE",
      Cutoff = cut,
      Accuracy = cm$overall["Accuracy"],
      Recall = recall,
      F1 = F1
    )
  )
}

results_smote
```

```{r}
# Cargar datos completos
datos_test <- read.csv("datos_relevantes_Test.csv")


# Guardar ID
id_test <- datos_test$ID


# --- MUY IMPORTANTE ---
# Aplicar EXACTAMENTE el mismo preprocesado que train
# 1. Convertir las columnas factor al mismo tipo
for (col in factor_cols) {
  if (col %in% names(datos_test)) {
    datos_test[[col]] <- as.factor(datos_test[[col]])
  }
}

# 2. Convertir todo lo demás a numérico
for (col in setdiff(names(datos_test), factor_cols)) {
  datos_test[[col]] <- as.numeric(datos_test[[col]])
}

# Predicción con XGB
probs_submission <- predict(modelo_smote3, newdata = datos_test, type = "prob")[,"1"]
pred_submission <- ifelse(probs_submission > 0.335,"1", "0")

# Crear submission
submission <- data.frame(
  ID = id_test,
  Exited = pred_submission
)

submission

#write.csv(submission, "Boosting_XGB_SMOTE_0.325_Decision3.csv", row.names = FALSE)
```

#AR

```{r}
bank_data<-read.csv("datos_missing_train.csv", header = TRUE, stringsAsFactors = TRUE)


bank_data <- bank_data %>%
  filter(!is.na(Exited)) %>%
  select(-ID)


bank_data$Tenure <- cut(bank_data$Tenure, breaks=c(-1,3,7,10), labels=c("Corto","Medio","Largo"))
bank_data$NetPromoterScore <- cut(bank_data$NetPromoterScore, breaks=c(-1,3,7,10),labels=c("Insatisfecho","Satisfecho","Contento"))
bank_data$TransactionFrequency <- cut(bank_data$TransactionFrequency, breaks=c(0,25,35,58), labels=c("Baja","Media","Alta"))
bank_data$Age <- cut(bank_data$Age, breaks=c(0,30,45,92), labels=c("Joven","Adulto","Mayor"))
bank_data$ComplaintsCount <- ifelse(bank_data$ComplaintsCount>0, "ConReclamos", "SinReclamos")
bank_data$ComplaintsCount<-factor(bank_data$ComplaintsCount)
bank_data$EstimatedSalary <- cut(bank_data$EstimatedSalary, breaks=c(0,50000,100000,150000,199992.48),labels=c("Bajo","MedioBajo","MedioAlto","Alto"))
bank_data$AvgTransactionAmount <- cut(bank_data$AvgTransactionAmount, breaks=c(0,150,611.35), labels=c("Baja","Alta"))
bank_data$DigitalEngagementScore<- cut(bank_data$DigitalEngagementScore, breaks=c(0,50,75,100), labels=c("Bajo","Medio","Alto"))
bank_data$CreditScore <- cut(bank_data$CreditScore, breaks=c(0,600,750,850), labels=c("Bajo","Medio","Alto"))
bank_data$Balance <- cut(bank_data$Balance, breaks=c(-1,1,150000,250898.09),labels=c("Sin Saldo","Medio","Alto"))
bank_data$NumOfProducts<- cut(bank_data$NumOfProducts, breaks=c(0,2,4),right=FALSE,labels=c("Un Producto","Más de uno"))

# Convertimos las logicas

bank_data <- bank_data %>%
  mutate(
    HasCrCard = factor(HasCrCard, levels = c(0, 1), labels = c("No", "Si")),
    SavingsAccountFlag = factor(SavingsAccountFlag, levels = c(0, 1), labels = c("No", "Si")),
    IsActiveMember = factor(IsActiveMember, levels = c(0, 1), labels = c("No", "Si")),
    Exited = factor(Exited, levels = c(0, 1), labels = c("0", "1"))
  )
```

```{r}
# CONVERTIR EN OBJETO DE TRANSACCIONES 
ttr_bank <- as(bank_data, "transactions")
ttr_bank
summary(ttr_bank)

# ANALIZAR EL NÚMERO DE ITEMS POR TRANSACCIÓN
inspect(ttr_bank[1:6])
SIZE <- size(ttr_bank)
summary(SIZE)
data.frame(SIZE) %>%
  ggplot(aes(x = SIZE)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Distribución del tamaño de las transacciones", x = "Número de items", y = "Frecuencia") +
  theme_bw()
itemFrequencyPlot(ttr_bank, topN = 20, type = "absolute", cex.names = 0.7)
```

```{r}
# Ajusta los parámetros según la densidad del dataset:
rules_bank <- apriori(
  ttr_bank,
  parameter = list(supp = 0.01, conf = 0.6, minlen = 1, maxlen = 5)
)

summary(rules_bank)
inspect(head(rules_bank, 10))

# ORDENAMOS las REGLAS segun lift y confianza
rules_lift <- sort(rules_bank, by = "lift", decreasing = TRUE)
inspect(head(rules_lift, 10))
rules_conf <- sort(rules_bank, by = "confidence", decreasing = TRUE)
inspect(head(rules_conf, 10))

# FILTRAR REGLAS ESPECÍFICAS 
# Ejemplo: clientes que abandonaron el banco (Exited=1)
rules_exited <- subset(rules_bank, subset = rhs %in% "Exited=1")
inspect(sort(rules_exited, by = "lift"))
```

```{r}
aplicar_reglas_churn <- function(pred_df, reglas, target_col_model="Exited_model") {

  df <- pred_df
  df$Exited_model <- as.character(df[[target_col_model]])

  df_rules <- df

  # Predicción final inicial = modelo
  df$Exited_final <- df$Exited_model

  # CAMBIO CLAVE AQUÍ 
  lhs_list <- LIST(lhs(reglas))  

  for (i in seq_along(lhs_list)) {

    items <- lhs_list[[i]]
    mask <- rep(TRUE, nrow(df_rules))

    for (it in items) {

      partes <- strsplit(it, "=")[[1]]
      var <- partes[1]
      val <- gsub('"', '', partes[2])

      if (var %in% names(df_rules)) {
        mask <- mask & as.character(df_rules[[var]]) == val
      } else {
        mask <- rep(FALSE, length(mask))
        break
      }
    }

    # Si la regla se cumple → forzar churn
    df$Exited_final[mask] <- "1"
  }

  df$Exited_final <- factor(df$Exited_final, levels = c("0","1"))
  df
}

```

```{r}
pred_df <- submission
datostesttotales<-read.csv("datos_missing_test.csv")

datostesttotales$Tenure <- cut(datostesttotales$Tenure, breaks=c(-1,3,7,10), labels=c("Corto","Medio","Largo"))
datostesttotales$NetPromoterScore <- cut(datostesttotales$NetPromoterScore, breaks=c(-1,3,7,10),labels=c("Insatisfecho","Satisfecho","Contento"))
datostesttotales$TransactionFrequency <- cut(datostesttotales$TransactionFrequency, breaks=c(0,25,35,58), labels=c("Baja","Media","Alta"))
datostesttotales$Age <- cut(datostesttotales$Age, breaks=c(0,30,45,92), labels=c("Joven","Adulto","Mayor"))
datostesttotales$ComplaintsCount <- ifelse(datostesttotales$ComplaintsCount>0, "ConReclamos", "SinReclamos")
datostesttotales$ComplaintsCount<-factor(datostesttotales$ComplaintsCount)
datostesttotales$EstimatedSalary <- cut(datostesttotales$EstimatedSalary, breaks=c(0,50000,100000,150000,199992.48),labels=c("Bajo","MedioBajo","MedioAlto","Alto"))
datostesttotales$AvgTransactionAmount <- cut(datostesttotales$AvgTransactionAmount, breaks=c(0,150,611.35), labels=c("Baja","Alta"))
datostesttotales$DigitalEngagementScore<- cut(datostesttotales$DigitalEngagementScore, breaks=c(0,50,75,100), labels=c("Bajo","Medio","Alto"))
datostesttotales$CreditScore <- cut(datostesttotales$CreditScore, breaks=c(0,600,750,850), labels=c("Bajo","Medio","Alto"))
datostesttotales$Balance <- cut(datostesttotales$Balance, breaks=c(-1,1,150000,250898.09),labels=c("Sin Saldo","Medio","Alto"))
datostesttotales$NumOfProducts<- cut(datostesttotales$NumOfProducts, breaks=c(0,2,4),right=FALSE,labels=c("Un Producto","Más de uno"))

# Convertimos las logicas

datostesttotales <- datostesttotales %>%
  mutate(
    HasCrCard = factor(HasCrCard, levels = c(0, 1), labels = c("No", "Si")),
    SavingsAccountFlag = factor(SavingsAccountFlag, levels = c(0, 1), labels = c("No", "Si")),
    IsActiveMember = factor(IsActiveMember, levels = c(0, 1), labels = c("No", "Si")),
  )
pred_df <- submission %>%
  left_join(datostesttotales, by = "ID")


pred_df$Exited <- factor(pred_df$Exited, levels=c("0","1"))

pred_df_rules <- aplicar_reglas_churn(
  pred_df = pred_df,
  reglas = rules_exited,
  target_col_model = "Exited"
)

submissionAR <- pred_df_rules %>%
  transmute(
    ID = ID,
    Exited = Exited_final
  )


submissionAR

table(submissionAR$Exited)

# Filas donde el modelo original era 0 y las reglas cambiaron a 1
cambios <- pred_df_rules %>%
  mutate(cambio = ifelse(Exited == "0" & Exited_final == "1", TRUE, FALSE)) %>%
  filter(cambio == TRUE)

# Ver IDs de los cambios
cambios$ID

# Ver todo el detalle de las observaciones cambiadas
cambios

submissionAR <- submissionAR %>%
  mutate(Exited = factor(Exited, levels = c(0, 1), labels = c("No", "Yes")))

#write.csv(submissionAR, "Boosting_XGB_SMOTE_0.335_Decision3_AR.csv", row.names = FALSE)
```


